<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Policy Gradient Neural Network Demo</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- React and ReactDOM -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    
    <!-- Babel Standalone for JSX -->
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    
    <style>
        .demo-container {
            background: linear-gradient(135deg, #dbeafe 0%, #fdf4ff 100%);
            min-height: 100vh;
        }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useCallback, useRef } = React;

        // Simple icon components to replace Lucide icons
        const PlayIcon = () => (
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
                <polygon points="5,3 19,12 5,21"></polygon>
            </svg>
        );

        const PauseIcon = () => (
            <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
                <rect x="6" y="4" width="4" height="16"></rect>
                <rect x="14" y="4" width="4" height="16"></rect>
            </svg>
        );

        const RotateCcwIcon = () => (
            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
                <polyline points="1,4 1,10 7,10"></polyline>
                <path d="M3.51,15a9,9,0,0,0,14.85-3.36,9,9,0,0,0-13.34-8.64L1,10"></path>
            </svg>
        );

        const BrainIcon = () => (
            <svg width="32" height="32" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
                <path d="M9.5,2A2.5,2.5,0,0,1,12,4.5v15a2.5,2.5,0,0,1-5,0V16.5a2.5,2.5,0,0,1-2.5-2.5v-2A2.5,2.5,0,0,1,7,9.5V7A2.5,2.5,0,0,1,9.5,2Z"></path>
                <path d="M14.5,2A2.5,2.5,0,0,0,12,4.5v15a2.5,2.5,0,0,0,5,0V16.5a2.5,2.5,0,0,0,2.5-2.5v-2A2.5,2.5,0,0,0,17,9.5V7A2.5,2.5,0,0,0,14.5,2Z"></path>
            </svg>
        );

        const NavigationIcon = () => (
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
                <polygon points="3,11 22,2 13,21 11,13 3,11"></polygon>
            </svg>
        );

        const TrendingUpIcon = () => (
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
                <polyline points="22,7 13.5,15.5 8.5,10.5 2,17"></polyline>
                <polyline points="16,7 22,7 22,13"></polyline>
            </svg>
        );

        const LayersIcon = () => (
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
                <polygon points="12,2 2,7 12,12 22,7 12,2"></polygon>
                <polyline points="2,17 12,22 22,17"></polyline>
                <polyline points="2,12 12,17 22,12"></polyline>
            </svg>
        );

        // Define main component for the Policy Gradient Demo
const NeuralNetworkDemo = () => {
  const canvasRef = useRef(null);
  
  // Environment parameters - simplified
  const CANVAS_SIZE = 400;
  const MAX_STEP = 10; // Increased maximum step size for faster movement
  const GOAL_RADIUS = 20;
  const AGENT_RADIUS = 8;
  
  // Neural network architecture
  const HIDDEN_SIZE = 16;
  const INPUT_SIZE = 4; // Simplified state
  const OUTPUT_SIZE = 4; // [mean_dx, mean_dy, log_std_dx, log_std_dy]
  
  // Xavier initialization for better convergence
  const xavierInit = (inputSize, outputSize) => 
    Array.from({ length: outputSize }, () => 
      Array.from({ length: inputSize }, () => 
        (Math.random() - 0.5) * Math.sqrt(6 / (inputSize + outputSize))
      )
    );
  
  const initializeNetwork = () => ({
    // Input to hidden layer
    w1: xavierInit(INPUT_SIZE, HIDDEN_SIZE),
    b1: Array.from({ length: HIDDEN_SIZE }, () => 0),
    
    // Hidden to output layer
    w2: xavierInit(HIDDEN_SIZE, OUTPUT_SIZE),
    b2: Array.from({ length: OUTPUT_SIZE }, () => 0)
  });

  const [network, setNetwork] = useState(initializeNetwork);
  const [agentPos, setAgentPos] = useState({ x: 50, y: 50 });
  const [goalPos, setGoalPos] = useState({ x: 350, y: 350 });
  const [episode, setEpisode] = useState(0);
  const [isTraining, setIsTraining] = useState(false);
  const [trajectory, setTrajectory] = useState([]);
  const [episodeReward, setEpisodeReward] = useState(0);
  const [rewardHistory, setRewardHistory] = useState([]);
  const [baseline, setBaseline] = useState(0);
  const [advantageHistory, setAdvantageHistory] = useState([]);
  const [gradientNorms, setGradientNorms] = useState([]);
  const [policyEntropy, setPolicyEntropy] = useState([]);
  const [baseLearningRate] = useState(0.01); // Increased base learning rate
  const [lastBoundaryCollision, setLastBoundaryCollision] = useState(null); // Track the last boundary collision
  
  // Add controller mode state (Neural Network or PID)
  const [controllerMode, setControllerMode] = useState('neural'); // 'neural' or 'pid'
  const [pidParams, setPidParams] = useState({
    kp: 0.2,   // Proportional gain
    ki: 0.01,  // Integral gain
    kd: 0.1,   // Derivative gain
    maxSpeed: MAX_STEP * 0.8 // Max speed slightly lower than neural network
  });
  
  // Function to update PID parameters
  const updatePidParam = (param, value) => {
    setPidParams(prev => ({
      ...prev,
      [param]: parseFloat(value)
    }));
  };
  
  // Add experience replay buffer for more efficient learning
  const [experienceBuffer, setExperienceBuffer] = useState([]);
  const BUFFER_SIZE = 5; // Store recent episodes for replay
  
  // Debug state variables
  const [debugValues, setDebugValues] = useState({
    meanAdvantage: 0,
    stdAdvantage: 0,
    meanReward: 0, 
    meanStdDev: 0,
    actionMagnitude: 0,
    successRate: 0,
    successes: 0,
    episodes: 0,
    boundaryTerminations: 0, // Track boundary terminations
    // PID controller specific metrics
    pidIntegralTerm: 0,
    pidErrorRate: 0
  });
  
  // Add momentum buffer for more stable updates
  const [momentumBuffer, setMomentumBuffer] = useState({
    w1: null,
    b1: null,
    w2: null,
    b2: null
  });

  // Activation functions
  const relu = (x) => Math.max(0, x);
  const reluDerivative = (x) => x > 0 ? 1 : 0;
  const tanh = (x) => Math.tanh(x);
  const tanhDerivative = (x) => 1 - Math.tanh(x) ** 2;

  // Improved Gaussian sampling using Box-Muller transform
  let spare = null;
  const gaussianRandom = () => {
    if (spare !== null) {
      const tmp = spare;
      spare = null;
      return tmp;
    }
    
    const u1 = Math.random();
    const u2 = Math.random();
    const mag = Math.sqrt(-2 * Math.log(u1));
    spare = mag * Math.cos(2 * Math.PI * u2);
    return mag * Math.sin(2 * Math.PI * u2);
  };

  // Forward pass - simplified for position control
  const forwardPass = (state) => {
    // Validate input state
    if (!state || state.some(s => !isFinite(s))) {
      console.warn('Invalid state detected, using safe defaults');
      state = [0, 0, 0, 0];
    }
    
    const { w1, b1, w2, b2 } = network;
    
    // Input to hidden layer
    const hidden = w1.map((weights, i) => {
      let z = b1[i];
      for (let j = 0; j < state.length; j++) {
        if (isFinite(weights[j]) && isFinite(state[j])) {
          z += weights[j] * state[j];
        }
      }
      z = Math.max(-8, Math.min(8, z));
      return relu(z);
    });
    
    // Hidden to output layer
    const output = w2.map((weights, i) => {
      let z = b2[i];
      for (let j = 0; j < hidden.length; j++) {
        if (isFinite(weights[j]) && isFinite(hidden[j])) {
          z += weights[j] * hidden[j];
        }
      }
      z = Math.max(-3, Math.min(3, z));
      // Initialize with higher values for the standard deviation outputs
      if (i >= 2 && episode < 5) {
        z = 0.5; // Start with higher standard deviation
      }
      return z;
    });
    
    // Process outputs - direct step commands
    let meanDx = tanh(output[0]) * MAX_STEP;
    let meanDy = tanh(output[1]) * MAX_STEP;
    
    // Exploration scheduling with forced minimum exploration
    // This ensures we maintain sufficient exploration throughout training
    const explorationSchedule = (ep) => {
      // Start with high exploration and slowly decay
      // This function returns a value between 1.0 (start) and 0.2 (after many episodes)
      return Math.max(0.2, Math.min(1.0, 2000 / (ep + 2000)));
    };
    
    // Calculate exploration factor for this episode
    const explorationFactor = explorationSchedule(episode);
    
    // Force higher exploration early, gradually reducing over time
    // Minimum log std is higher (less negative) early in training for more exploration
    const minLogStd = -2.0 * (1.0 - explorationFactor) - 0.1 * explorationFactor;
    
    // Maximum log std also changes with training - wider early, narrower later
    const maxLogStd = 0.9 * explorationFactor + 0.3 * (1.0 - explorationFactor);
    
    // Apply the bounds to the network outputs
    let logStdDx = Math.max(minLogStd, Math.min(maxLogStd, output[2])); 
    let logStdDy = Math.max(minLogStd, Math.min(maxLogStd, output[3]));
    
    // Safety checks
    if (!isFinite(meanDx)) meanDx = 0;
    if (!isFinite(meanDy)) meanDy = 0;
    if (!isFinite(logStdDx)) logStdDx = -0.5;
    if (!isFinite(logStdDy)) logStdDy = -0.5;
    
    return {
      meanDx,
      meanDy,
      logStdDx,
      logStdDy,
      hidden,
      rawOutput: output
    };
  };

  // Sample action - now for position steps
  const sampleAction = (mean, logStd) => {
    const std = Math.exp(logStd);
    const noise = gaussianRandom();
    return Math.max(-MAX_STEP, Math.min(MAX_STEP, mean + noise * std));
  };

  // Log probability
  const logProb = (action, mean, logStd) => {
    const std = Math.exp(logStd);
    const diff = action - mean;
    return -0.5 * Math.log(2 * Math.PI) - logStd - 0.5 * (diff * diff) / (std * std);
  };

  // Calculate policy entropy
  const calculateEntropy = (logStdDx, logStdDy) => {
    return 0.5 * (1 + Math.log(2 * Math.PI)) + logStdDx + 
           0.5 * (1 + Math.log(2 * Math.PI)) + logStdDy;
  };

  // Simplified state representation
  const getState = (pos, goal) => {
    const dx = goal.x - pos.x;
    const dy = goal.y - pos.y;
    const distance = Math.sqrt(dx * dx + dy * dy);
    const maxDistance = Math.sqrt(CANVAS_SIZE ** 2 + CANVAS_SIZE ** 2);
    
    return [
      pos.x / CANVAS_SIZE - 0.5,        // Normalized position [-0.5, 0.5]
      pos.y / CANVAS_SIZE - 0.5,
      dx / maxDistance,                 // Normalized direction to goal
      dy / maxDistance
    ];
  };

  // Enhanced reward function with better shaping
  const getReward = (pos, goal, prevPos) => {
    // Ensure all inputs are finite
    if (!isFinite(pos.x) || !isFinite(pos.y) || 
        !isFinite(goal.x) || !isFinite(goal.y)) {
      return -5;
    }
    
    const distance = Math.sqrt((pos.x - goal.x) ** 2 + (pos.y - goal.y) ** 2);
    if (!isFinite(distance)) return -5;
    
    let reward = 0;
    
    // Goal reached - significant reward but not overwhelming
    if (distance < GOAL_RADIUS) {
      reward += 30; // Balanced reward for reaching goal
    }
    
    // Progress reward - highly emphasize moving toward the goal
    if (prevPos) {
      const prevDistance = Math.sqrt((prevPos.x - goal.x) ** 2 + (prevPos.y - goal.y) ** 2);
      const progress = prevDistance - distance;
      
      // Exponential progress reward - gives more reward as agent gets closer
      // This creates a stronger gradient near the goal
      const progressMultiplier = 10 * (1 + 5 * Math.exp(-distance / 50));
      reward += progress * progressMultiplier;
      
      // Additional reward for consistent progress
      if (progress > 0) {
        reward += 0.5; // Small bonus for any positive progress
      }
    }
    
    // Distance reward - with smoother gradient using exponential scaling
    const maxDistance = Math.sqrt(CANVAS_SIZE ** 2 + CANVAS_SIZE ** 2);
    const normalizedDistance = distance / maxDistance;
    
    // This creates a more aggressive reward curve that emphasizes getting closer to the goal
    reward += 12 * Math.exp(-3 * normalizedDistance);
    
    // Smaller time penalty to avoid rushing
    reward -= 0.03; // Very small penalty per step
    
    // Boundary penalty - keep agent in bounds but not too harsh
    const margin = 10;
    if (pos.x < margin || pos.x > CANVAS_SIZE - margin || 
        pos.y < margin || pos.y > CANVAS_SIZE - margin) {
      reward -= 8; // Increased penalty when agent is near boundaries
    }
    
    // Safety check
    if (!isFinite(reward)) return -1;
    
    return Math.max(-15, Math.min(40, reward)); // Clamped reward
  };

  // Simple physics - direct position update
  const updatePhysics = (pos, action) => {
    // Direct position update
    let newPos = {
      x: pos.x + action.x,
      y: pos.y + action.y
    };
    
    // Keep in bounds
    newPos.x = Math.max(AGENT_RADIUS, Math.min(CANVAS_SIZE - AGENT_RADIUS, newPos.x));
    newPos.y = Math.max(AGENT_RADIUS, Math.min(CANVAS_SIZE - AGENT_RADIUS, newPos.y));
    
    return newPos;
  };
  
  // PID Controller implementation
  const pidController = (() => {
    // Error history for integral term
    let errorSumX = 0;
    let errorSumY = 0;
    
    // Previous errors for derivative term
    let lastErrorX = 0;
    let lastErrorY = 0;
    
    // Reset PID state
    const reset = () => {
      errorSumX = 0;
      errorSumY = 0;
      lastErrorX = 0;
      lastErrorY = 0;
    };
    
    // Calculate PID control action
    const getAction = (currentPos, goalPos) => {
      // Calculate error (distance to goal)
      const errorX = goalPos.x - currentPos.x;
      const errorY = goalPos.y - currentPos.y;
      
      // Update integral term (with anti-windup)
      errorSumX = Math.max(-100, Math.min(100, errorSumX + errorX));
      errorSumY = Math.max(-100, Math.min(100, errorSumY + errorY));
      
      // Calculate derivative term
      const derivativeX = errorX - lastErrorX;
      const derivativeY = errorY - lastErrorY;
      
      // Save current error for next iteration
      lastErrorX = errorX;
      lastErrorY = errorY;
      
      // Calculate PID outputs
      let actionX = pidParams.kp * errorX + pidParams.ki * errorSumX + pidParams.kd * derivativeX;
      let actionY = pidParams.kp * errorY + pidParams.ki * errorSumY + pidParams.kd * derivativeY;
      
      // Normalize to maintain max speed
      const magnitude = Math.sqrt(actionX * actionX + actionY * actionY);
      if (magnitude > pidParams.maxSpeed) {
        const scale = pidParams.maxSpeed / magnitude;
        actionX *= scale;
        actionY *= scale;
      }
      
      // Ensure we don't exceed MAX_STEP
      actionX = Math.max(-MAX_STEP, Math.min(MAX_STEP, actionX));
      actionY = Math.max(-MAX_STEP, Math.min(MAX_STEP, actionY));
      
      // Update debug metrics for PID controller
      if (controllerMode === 'pid') {
        setDebugValues(prev => ({
          ...prev,
          pidIntegralTerm: Math.sqrt(errorSumX * errorSumX + errorSumY * errorSumY),
          pidErrorRate: Math.sqrt(derivativeX * derivativeX + derivativeY * derivativeY)
        }));
      }
      
      return { x: actionX, y: actionY };
    };
    
    // Get current state of the controller for debugging
    const getState = () => ({
      errorSumX,
      errorSumY,
      lastErrorX,
      lastErrorY
    });
    
    return { getAction, reset, getState };
  })();

  // Simplified episode generation
  const runEpisode = useCallback(() => {
    const episodeTrajectory = [];
    
    // More varied starting positions to encourage generalization
    // Randomly place the agent in different locations on the canvas
    let currentPos;
    
    // Start with restricted random positions early in training
    if (episode < 100) {
      // Early in training - start in the top-left quarter to make it easier
      currentPos = { 
        x: 50 + Math.random() * 150, 
        y: 50 + Math.random() * 150
      };
    } else if (episode < 300) {
      // Mid training - start in a larger area
      currentPos = { 
        x: 30 + Math.random() * 250, 
        y: 30 + Math.random() * 250
      };
    } else {
      // Later in training - start anywhere on the canvas
      currentPos = { 
        x: 20 + Math.random() * (CANVAS_SIZE - 40), 
        y: 20 + Math.random() * (CANVAS_SIZE - 40)
      };
    }
    let totalReward = 0;
    let prevPos = null;
    const maxSteps = 150; // Increased from 100 to 150 steps
    const gamma = 0.99;

    for (let step = 0; step < maxSteps; step++) {
      const state = getState(currentPos, goalPos);
      
      // Choose action based on controller mode
      let action, policyOutput, logProbX, logProbY, entropy;
      
      if (controllerMode === 'neural') {
        // Neural network policy
        policyOutput = forwardPass(state);
        const actionX = sampleAction(policyOutput.meanDx, policyOutput.logStdDx);
        const actionY = sampleAction(policyOutput.meanDy, policyOutput.logStdDy);
        action = { x: actionX, y: actionY };
        
        logProbX = logProb(actionX, policyOutput.meanDx, policyOutput.logStdDx);
        logProbY = logProb(actionY, policyOutput.meanDy, policyOutput.logStdDy);
        entropy = calculateEntropy(policyOutput.logStdDx, policyOutput.logStdDy);
      } else {
        // PID controller
        action = pidController.getAction(currentPos, goalPos);
        
        // Create dummy values for compatibility with trajectory format
        policyOutput = {
          meanDx: action.x,
          meanDy: action.y,
          logStdDx: -5, // Very low standard deviation (deterministic)
          logStdDy: -5
        };
        logProbX = 0;
        logProbY = 0;
        entropy = 0;
      }
      
      const nextPos = updatePhysics(currentPos, action);
      const reward = getReward(nextPos, goalPos, prevPos);
      
      episodeTrajectory.push({
        state,
        action,
        reward,
        logProbs: { x: logProbX, y: logProbY },
        position: { ...currentPos },
        policyOutput,
        entropy
      });
      
      totalReward += reward;
      prevPos = { ...currentPos };
      currentPos = nextPos;
      
      // Check if reached goal
      if (Math.sqrt((currentPos.x - goalPos.x) ** 2 + (currentPos.y - goalPos.y) ** 2) < GOAL_RADIUS) {
        // Update success tracking
        setDebugValues(prev => ({
          ...prev,
          successes: prev.successes + 1
        }));
        break;
      }
      
      // Check if the agent touches the boundary (new condition for termination)
      const boundaryMargin = 5; // Stricter margin for termination than penalty
      if (currentPos.x <= boundaryMargin || currentPos.x >= CANVAS_SIZE - boundaryMargin || 
          currentPos.y <= boundaryMargin || currentPos.y >= CANVAS_SIZE - boundaryMargin) {
        // Apply a stronger negative reward for boundary termination
        const boundaryPenalty = -15;
        totalReward += boundaryPenalty;
        
        // Add the penalty to the last step in the trajectory
        if (episodeTrajectory.length > 0) {
          episodeTrajectory[episodeTrajectory.length - 1].reward += boundaryPenalty;
        }
        
        // Track boundary terminations
        setDebugValues(prev => ({
          ...prev,
          boundaryTerminations: prev.boundaryTerminations + 1
        }));
        
        // Record the boundary collision location and timestamp for visualization
        setLastBoundaryCollision({
          position: { ...currentPos },
          time: Date.now(),
          side: currentPos.x <= boundaryMargin ? 'left' : 
                currentPos.x >= CANVAS_SIZE - boundaryMargin ? 'right' : 
                currentPos.y <= boundaryMargin ? 'top' : 'bottom'
        });
        
        break; // Terminate the episode
      }
    }
    
    // Calculate discounted returns
    const returns = [];
    let G = 0;
    for (let t = episodeTrajectory.length - 1; t >= 0; t--) {
      G = episodeTrajectory[t].reward + gamma * G;
      returns.unshift(G);
    }
    
    episodeTrajectory.forEach((step, i) => {
      step.return = returns[i];
    });
    
    return { trajectory: episodeTrajectory, totalReward };
  }, [network, goalPos, controllerMode]);

  // Enhanced canvas drawing
  const draw = useCallback((ctx, trajectory) => {
    // Clear with gradient background
    const gradient = ctx.createLinearGradient(0, 0, CANVAS_SIZE, CANVAS_SIZE);
    gradient.addColorStop(0, '#f8fafc');
    gradient.addColorStop(1, '#f1f5f9');
    ctx.fillStyle = gradient;
    ctx.fillRect(0, 0, CANVAS_SIZE, CANVAS_SIZE);
    
    // Draw grid
    ctx.strokeStyle = '#e2e8f0';
    ctx.lineWidth = 1;
    for (let i = 0; i <= CANVAS_SIZE; i += 50) {
      ctx.beginPath();
      ctx.moveTo(i, 0);
      ctx.lineTo(i, CANVAS_SIZE);
      ctx.stroke();
      ctx.beginPath();
      ctx.moveTo(0, i);
      ctx.lineTo(CANVAS_SIZE, i);
      ctx.stroke();
    }
    
    // Draw warning border to visualize boundary zone
    const boundaryMargin = 5;
    ctx.strokeStyle = 'rgba(239, 68, 68, 0.4)'; // Transparent red
    ctx.lineWidth = 2;
    ctx.strokeRect(
      boundaryMargin, 
      boundaryMargin, 
      CANVAS_SIZE - boundaryMargin * 2, 
      CANVAS_SIZE - boundaryMargin * 2
    );
    
    // Draw trajectory with reward-based coloring
    if (trajectory.length > 1) {
      for (let i = 1; i < trajectory.length; i++) {
        const prev = trajectory[i - 1];
        const curr = trajectory[i];
        const reward = curr.reward;
        
        // Color gradient based on reward
        let color;
        if (reward > 10) color = '#10b981'; // Bright green for excellent
        else if (reward > 5) color = '#34d399'; // Light green for good
        else if (reward > 0) color = '#3b82f6'; // Blue for okay
        else if (reward > -5) color = '#f59e0b'; // Orange for poor
        else color = '#ef4444'; // Red for bad
        
        ctx.strokeStyle = color;
        ctx.lineWidth = 3;
        ctx.beginPath();
        ctx.moveTo(prev.position.x, prev.position.y);
        ctx.lineTo(curr.position.x, curr.position.y);
        ctx.stroke();
      }
      
      // Draw position markers
      trajectory.forEach(({ position }, i) => {
        if (i % 12 === 0 && position && 
            isFinite(position.x) && isFinite(position.y)) {
          ctx.fillStyle = 'rgba(16, 185, 129, 0.5)';
          ctx.beginPath();
          ctx.arc(position.x, position.y, 2, 0, 2 * Math.PI);
          ctx.fill();
        }
      });
    }
    
    // Visual indicator for recent boundary collision
    if (lastBoundaryCollision && Date.now() - lastBoundaryCollision.time < 2000) {
      // Calculate fade effect based on time since collision
      const fadeAlpha = Math.max(0, 1 - (Date.now() - lastBoundaryCollision.time) / 2000);
      
      // Draw warning flash at collision point
      const pos = lastBoundaryCollision.position;
      if (isFinite(pos.x) && isFinite(pos.y)) {
        // Outer warning glow
        ctx.fillStyle = `rgba(239, 68, 68, ${fadeAlpha * 0.3})`;
        ctx.beginPath();
        ctx.arc(pos.x, pos.y, 25, 0, 2 * Math.PI);
        ctx.fill();
        
        // Inner warning circle
        ctx.fillStyle = `rgba(239, 68, 68, ${fadeAlpha * 0.7})`;
        ctx.beginPath();
        ctx.arc(pos.x, pos.y, 15, 0, 2 * Math.PI);
        ctx.fill();
        
        // Center warning symbol (X)
        ctx.strokeStyle = `rgba(255, 255, 255, ${fadeAlpha})`;
        ctx.lineWidth = 3;
        ctx.beginPath();
        ctx.moveTo(pos.x - 8, pos.y - 8);
        ctx.lineTo(pos.x + 8, pos.y + 8);
        ctx.moveTo(pos.x + 8, pos.y - 8);
        ctx.lineTo(pos.x - 8, pos.y + 8);
        ctx.stroke();
        
        // Draw "Boundary!" text
        ctx.font = '14px Arial';
        ctx.textAlign = 'center';
        ctx.fillStyle = `rgba(239, 68, 68, ${fadeAlpha})`;
        ctx.fillText('Boundary!', pos.x, pos.y + 35);
      }
    }
    
    // Draw goal with enhanced pulsing
    const time = Date.now() / 1000;
    const pulse = 1 + 0.15 * Math.sin(time * 4);
    
    // Ensure goal position is valid
    const safeGoalX = Math.max(0, Math.min(CANVAS_SIZE, goalPos.x || 0));
    const safeGoalY = Math.max(0, Math.min(CANVAS_SIZE, goalPos.y || 0));
    
    // Goal glow effect (with bounds checking)
    if (isFinite(safeGoalX) && isFinite(safeGoalY)) {
      try {
        const glowGradient = ctx.createRadialGradient(
          safeGoalX, safeGoalY, 0,
          safeGoalX, safeGoalY, GOAL_RADIUS * 1.5
        );
        glowGradient.addColorStop(0, 'rgba(16, 185, 129, 0.3)');
        glowGradient.addColorStop(1, 'rgba(16, 185, 129, 0)');
        ctx.fillStyle = glowGradient;
        ctx.beginPath();
        ctx.arc(safeGoalX, safeGoalY, GOAL_RADIUS * 1.5, 0, 2 * Math.PI);
        ctx.fill();
      } catch (e) {
        // Fallback to simple goal rendering
        ctx.fillStyle = 'rgba(16, 185, 129, 0.3)';
        ctx.beginPath();
        ctx.arc(safeGoalX, safeGoalY, GOAL_RADIUS * 1.5, 0, 2 * Math.PI);
        ctx.fill();
      }
    }
    
    // Goal body
    ctx.fillStyle = '#10b981';
    ctx.beginPath();
    ctx.arc(safeGoalX, safeGoalY, GOAL_RADIUS * pulse, 0, 2 * Math.PI);
    ctx.fill();
    ctx.strokeStyle = '#065f46';
    ctx.lineWidth = 3;
    ctx.stroke();
    
    // Goal center
    ctx.fillStyle = '#065f46';
    ctx.beginPath();
    ctx.arc(safeGoalX, safeGoalY, 5, 0, 2 * Math.PI);
    ctx.fill();
    
    // Draw agent with enhanced styling
    const lastPos = trajectory.length > 0 ? trajectory[trajectory.length - 1].position : agentPos;
    
    // Ensure agent position is valid
    const safeAgentX = Math.max(0, Math.min(CANVAS_SIZE, lastPos.x || 0));
    const safeAgentY = Math.max(0, Math.min(CANVAS_SIZE, lastPos.y || 0));
    
    // Agent glow (with bounds checking)
    if (isFinite(safeAgentX) && isFinite(safeAgentY)) {
      try {
        const agentGlow = ctx.createRadialGradient(
          safeAgentX, safeAgentY, 0,
          safeAgentX, safeAgentY, AGENT_RADIUS * 2
        );
        // Use blue glow for neural network, orange glow for PID
        const glowColor = controllerMode === 'neural' ? 'rgba(59, 130, 246, 0.4)' : 'rgba(249, 115, 22, 0.4)';
        const glowFade = controllerMode === 'neural' ? 'rgba(59, 130, 246, 0)' : 'rgba(249, 115, 22, 0)';
        agentGlow.addColorStop(0, glowColor);
        agentGlow.addColorStop(1, glowFade);
        ctx.fillStyle = agentGlow;
        ctx.beginPath();
        ctx.arc(safeAgentX, safeAgentY, AGENT_RADIUS * 2, 0, 2 * Math.PI);
        ctx.fill();
      } catch (e) {
        // Fallback to simple agent rendering
        ctx.fillStyle = controllerMode === 'neural' ? 'rgba(59, 130, 246, 0.4)' : 'rgba(249, 115, 22, 0.4)';
        ctx.beginPath();
        ctx.arc(safeAgentX, safeAgentY, AGENT_RADIUS * 2, 0, 2 * Math.PI);
        ctx.fill();
      }
    }
    
    // Agent shadow
    ctx.fillStyle = 'rgba(0, 0, 0, 0.2)';
    ctx.beginPath();
    ctx.arc(safeAgentX + 2, safeAgentY + 2, AGENT_RADIUS, 0, 2 * Math.PI);
    ctx.fill();
    
    // Agent body
    ctx.fillStyle = controllerMode === 'neural' ? '#3b82f6' : '#f97316'; // Blue for neural, orange for PID
    ctx.beginPath();
    ctx.arc(safeAgentX, safeAgentY, AGENT_RADIUS, 0, 2 * Math.PI);
    ctx.fill();
    ctx.strokeStyle = controllerMode === 'neural' ? '#1e40af' : '#c2410c'; // Darker blue/orange for stroke
    ctx.lineWidth = 2;
    ctx.stroke();
    
    // Draw agent with direction indicator - simplified for position control
    if (trajectory.length > 0) {
      const lastStep = trajectory[trajectory.length - 1];
      if (lastStep.action && isFinite(lastStep.action.x) && isFinite(lastStep.action.y)) {
        const actionMagnitude = Math.sqrt(lastStep.action.x ** 2 + lastStep.action.y ** 2);
        if (actionMagnitude > 0.5) {
          const angle = Math.atan2(lastStep.action.y, lastStep.action.x);
          if (isFinite(angle)) {
            ctx.strokeStyle = '#ffffff';
            ctx.lineWidth = 3;
            ctx.beginPath();
            ctx.moveTo(safeAgentX, safeAgentY);
            ctx.lineTo(
              safeAgentX + Math.cos(angle) * (AGENT_RADIUS - 1),
              safeAgentY + Math.sin(angle) * (AGENT_RADIUS - 1)
            );
            ctx.stroke();
          }
        }
      }
    }
  }, [agentPos, goalPos, lastBoundaryCollision]);

  // Reset function for all state
  const reset = () => {
    setNetwork(initializeNetwork());
    setAgentPos({ x: 50, y: 50 });
    setEpisode(0);
    setTrajectory([]);
    setEpisodeReward(0);
    setRewardHistory([]);
    setBaseline(0);
    setAdvantageHistory([]);
    setGradientNorms([]);
    setPolicyEntropy([]);
    setIsTraining(false);
    setExperienceBuffer([]); // Clear experience buffer
    setLastBoundaryCollision(null); // Reset boundary collision tracking
    setDebugValues({
      meanAdvantage: 0,
      stdAdvantage: 0,
      meanReward: 0, 
      meanStdDev: 0,
      actionMagnitude: 0,
      successRate: 0,
      successes: 0,
      episodes: 0,
      boundaryTerminations: 0,
      pidIntegralTerm: 0,
      pidErrorRate: 0
    });
    
    // Don't reset controller mode by default to allow comparisons
    // Uncomment to reset controller mode: setControllerMode('neural');
    
    // Initialize momentum buffer
    const initMomentum = () => {
      const newNetwork = initializeNetwork();
      return {
        w1: newNetwork.w1.map(row => row.map(() => 0)),
        b1: newNetwork.b1.map(() => 0),
        w2: newNetwork.w2.map(row => row.map(() => 0)),
        b2: newNetwork.b2.map(() => 0)
      };
    };
    
    setMomentumBuffer(initMomentum());
    
    // Reset PID controller
    pidController.reset();
  };

  // Canvas rendering
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    draw(ctx, trajectory);
    
    // Set up animation frame to keep boundary collision visuals updated
    const animationId = requestAnimationFrame(function animate() {
      draw(ctx, trajectory);
      requestAnimationFrame(animate);
    });
    
    return () => cancelAnimationFrame(animationId);
  }, [trajectory, draw]);

  // Training loop
  useEffect(() => {
    if (!isTraining) return;
    
    const trainInterval = setInterval(() => {
      const { trajectory: newTrajectory, totalReward } = runEpisode();
      
      setTrajectory(newTrajectory);
      setEpisodeReward(totalReward);
      setRewardHistory(prev => [...prev.slice(-99), totalReward]);
      
      // Add the new episode to the experience buffer
      setExperienceBuffer(prev => {
        let newBuffer = [...prev, { trajectory: newTrajectory, totalReward }];
        if (newBuffer.length > BUFFER_SIZE) {
          newBuffer = newBuffer.slice(-BUFFER_SIZE);
        }
        return newBuffer;
      });
      
      // Update policy using current episode
      let updatedNetwork = updatePolicy(newTrajectory, totalReward);
      
      // Experience replay: also update with previous successful episodes (if available)
      if (episode > 20 && experienceBuffer.length > 0) {
        // Prioritize replaying successful episodes
        const successfulEpisodes = experienceBuffer.filter(ep => 
          ep.trajectory.some(step => {
            const pos = step.position;
            const goalDist = Math.sqrt((pos.x - goalPos.x) ** 2 + (pos.y - goalPos.y) ** 2);
            return goalDist < GOAL_RADIUS;
          })
        );
        
        // If there are successful episodes, replay one of them
        if (successfulEpisodes.length > 0) {
          const replayEpisode = successfulEpisodes[Math.floor(Math.random() * successfulEpisodes.length)];
          // Use a smaller learning rate for replay
          updatedNetwork = updatePolicy(replayEpisode.trajectory, replayEpisode.totalReward, 0.5);
        }
      }
      
      setNetwork(updatedNetwork);
      setEpisode(prev => prev + 1);
    }, 150);
    
    return () => clearInterval(trainInterval);
  }, [isTraining, runEpisode, experienceBuffer, goalPos]);

  const handleCanvasClick = (e) => {
    const canvas = canvasRef.current;
    const rect = canvas.getBoundingClientRect();
    const x = (e.clientX - rect.left) * (CANVAS_SIZE / rect.width);
    const y = (e.clientY - rect.top) * (CANVAS_SIZE / rect.height);
    setGoalPos({ x, y });
  };

  // Get current policy for display
  const getCurrentPolicy = () => {
    const sampleState = getState(agentPos, goalPos);
    return forwardPass(sampleState);
  };

  const currentPolicy = getCurrentPolicy();

  return (
    <div className="max-w-7xl mx-auto p-6 bg-gradient-to-br from-blue-50 to-purple-50 min-h-screen">
      <div className="bg-white rounded-xl shadow-lg p-6 mb-6">
        <div className="flex items-center gap-3 mb-4">
          <BrainIcon className="text-blue-600" />
          <h1 className="text-3xl font-bold text-gray-800">Policy Gradient Neural Network</h1>
        </div>
        
        <p className="text-gray-600 mb-4">
          This demo shows a policy gradient algorithm learning to navigate to a goal. 
          The agent now properly learns to avoid boundaries by receiving a strong negative reward and terminating episodes when touching the edge.
        </p>
        
        <div className="flex gap-4 mb-6">
          <button
            onClick={() => setIsTraining(!isTraining)}
            className={`flex items-center gap-2 px-4 py-2 rounded-lg font-medium transition-colors ${
              isTraining 
                ? 'bg-red-500 hover:bg-red-600 text-white' 
                : 'bg-green-500 hover:bg-green-600 text-white'
            }`}
          >
            {isTraining ? <PauseIcon /> : <PlayIcon />}
            {isTraining ? 'Pause' : 'Start'} Training
          </button>
          
          <button
            onClick={reset}
            className="flex items-center gap-2 px-4 py-2 bg-gray-500 hover:bg-gray-600 text-white rounded-lg font-medium transition-colors"
          >
            <RotateCcwIcon />
            Reset
          </button>
          
          {/* Controller Toggle */}
          <div className="flex items-center gap-2 ml-4">
            <span className="text-sm text-gray-600">Controller:</span>
            <div className="flex rounded-lg overflow-hidden border border-gray-300">
              <button
                onClick={() => setControllerMode('neural')}
                className={`px-3 py-1 text-sm font-medium transition-colors ${
                  controllerMode === 'neural'
                    ? 'bg-blue-500 text-white'
                    : 'bg-gray-100 hover:bg-gray-200 text-gray-700'
                }`}
              >
                Neural Network
              </button>
              <button
                onClick={() => setControllerMode('pid')}
                className={`px-3 py-1 text-sm font-medium transition-colors ${
                  controllerMode === 'pid'
                    ? 'bg-orange-500 text-white'
                    : 'bg-gray-100 hover:bg-gray-200 text-gray-700'
                }`}
              >
                PID Controller
              </button>
            </div>
          </div>
        </div>
      </div>

      <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
        {/* Environment */}
        <div className="lg:col-span-2 bg-white rounded-xl shadow-lg p-6">
          <h2 className="text-xl font-bold mb-4 flex items-center gap-2">
            <NavigationIcon className="text-blue-600" />
            Environment
          </h2>
          
          <canvas
            ref={canvasRef}
            width={CANVAS_SIZE}
            height={CANVAS_SIZE}
            onClick={handleCanvasClick}
            className="border-2 border-gray-300 rounded-lg cursor-crosshair w-full max-w-md mx-auto"
            style={{ aspectRatio: '1/1' }}
          />
          
          <div className="mt-4 text-sm text-gray-600 space-y-1">
            <div className="flex items-center gap-2">
              <div className="w-4 h-4 bg-blue-500 rounded-full"></div>
              <span>Agent ({controllerMode === 'neural' ? 'neural network' : 'PID controller'})</span>
            </div>
            <div className="flex items-center gap-2">
              <div className="w-4 h-4 bg-green-500 rounded-full"></div>
              <span>Goal (click to move)</span>
            </div>
            <div className="flex items-center gap-2">
              <div className="w-4 h-4 border-2 border-red-400 rounded-full bg-transparent"></div>
              <span>Boundary zone (episodes terminate here)</span>
            </div>
            <div className="grid grid-cols-2 gap-1 text-xs mt-2">
              <div className="flex items-center gap-1">
                <div className="w-3 h-1 bg-green-400"></div>
                <span>Excellent</span>
              </div>
              <div className="flex items-center gap-1">
                <div className="w-3 h-1 bg-blue-400"></div>
                <span>Good</span>
              </div>
              <div className="flex items-center gap-1">
                <div className="w-3 h-1 bg-orange-400"></div>
                <span>Poor</span>
              </div>
              <div className="flex items-center gap-1">
                <div className="w-3 h-1 bg-red-400"></div>
                <span>Bad</span>
              </div>
            </div>
            
            {/* Controller badge */}
            <div className={`mt-2 inline-block px-2 py-1 rounded text-xs text-white font-medium ${
              controllerMode === 'neural' ? 'bg-blue-500' : 'bg-orange-500'
            }`}>
              {controllerMode === 'neural' ? 'Neural Network Active' : 'PID Controller Active'}
            </div>
            
            {/* Episode info */}
            <div className="mt-3 grid grid-cols-2 gap-4">
              <div className="bg-blue-50 p-2 rounded">
                <div className="text-xs text-gray-600">Episode</div>
                <div className="font-bold text-blue-600">{episode}</div>
              </div>
              <div className="bg-green-50 p-2 rounded">
                <div className="text-xs text-gray-600">Last Reward</div>
                <div className="font-bold text-green-600">{episodeReward.toFixed(1)}</div>
              </div>
              <div className="bg-purple-50 p-2 rounded">
                <div className="text-xs text-gray-600">Success Rate</div>
                <div className="font-bold text-purple-600">{(debugValues.successRate * 100).toFixed(1)}%</div>
              </div>
              <div className="bg-red-50 p-2 rounded">
                <div className="text-xs text-gray-600">Boundary Terminations</div>
                <div className="font-bold text-red-600">{debugValues.boundaryTerminations}</div>
              </div>
            </div>
          </div>
        </div>

        {/* Control Panel */}
        <div className="space-y-6">
          <div className="bg-white rounded-xl shadow-lg p-6">
            <h2 className="text-xl font-bold mb-4">Current Policy</h2>
            
            {controllerMode === 'neural' ? (
              <div className="space-y-4">
                <div>
                  <h3 className="font-medium mb-2">Step Means:</h3>
                  <div className="grid grid-cols-1 gap-2 text-sm">
                    <div className="bg-blue-50 p-2 rounded">
                      <span className="text-gray-600">Dx:</span>
                      <span className="font-mono ml-1">{currentPolicy.meanDx.toFixed(3)}</span>
                    </div>
                    <div className="bg-blue-50 p-2 rounded">
                      <span className="text-gray-600">Dy:</span>
                      <span className="font-mono ml-1">{currentPolicy.meanDy.toFixed(3)}</span>
                    </div>
                  </div>
                </div>
                
                <div>
                  <h3 className="font-medium mb-2">Exploration:</h3>
                  <div className="grid grid-cols-1 gap-2 text-sm">
                    <div className="bg-purple-50 p-2 rounded">
                      <span className="text-gray-600">σx:</span>
                      <span className="font-mono ml-1">{Math.exp(currentPolicy.logStdDx).toFixed(3)}</span>
                    </div>
                    <div className="bg-purple-50 p-2 rounded">
                      <span className="text-gray-600">σy:</span>
                      <span className="font-mono ml-1">{Math.exp(currentPolicy.logStdDy).toFixed(3)}</span>
                    </div>
                  </div>
                </div>
              </div>
            ) : (
              <div className="space-y-4">
                <div className="bg-orange-50 p-3 rounded-lg mb-2">
                  <div className="text-sm text-gray-600 font-medium">PID Controller</div>
                  <div className="text-xs text-gray-600 mt-1">Classic proportional-integral-derivative control</div>
                </div>
              </div>
            )}
            
            {rewardHistory.length > 0 && (
              <div>
                <h3 className="font-medium mb-2 mt-4">Reward History:</h3>
                <div className="h-16 flex items-end gap-1">
                  {rewardHistory.slice(-20).map((reward, idx) => {
                    const maxReward = Math.max(...rewardHistory);
                    const minReward = Math.min(...rewardHistory);
                    const range = maxReward - minReward || 1;
                    const height = Math.max(2, ((reward - minReward) / range) * 50);
                    const isGood = reward > baseline;
                    return (
                      <div
                        key={idx}
                        className={`w-2 rounded-t ${isGood ? 'bg-green-400' : 'bg-red-400'}`}
                        style={{ height: `${height}px` }}
                        title={`${reward.toFixed(1)}`}
                      />
                    );
                  })}
                </div>
              </div>
            )}
          </div>

          {/* Improvements Summary */}
          <div className="bg-white rounded-xl shadow-lg p-6">
            <h2 className="text-xl font-bold mb-4">Learning Improvements</h2>
            
            <div className="space-y-3 text-sm">
              <div>
                <span className="font-medium text-blue-600">✓ Boundary Avoidance:</span>
                <p>Agent now learns to avoid boundaries with strong penalties and visual feedback</p>
              </div>
              
              <div>
                <span className="font-medium text-red-600">✓ Episode Termination:</span>
                <p>Episodes terminate when agent touches boundaries with a clear visual indicator</p>
              </div>
              
              <div>
                <span className="font-medium text-green-600">✓ Visible Boundaries:</span>
                <p>Red boundary zone clearly shows where episodes will terminate</p>
              </div>
              
              <div>
                <span className="font-medium text-purple-600">✓ Performance Tracking:</span>
                <p>Boundary collision counter helps monitor learning progress</p>
              </div>
            </div>
            
            <div className="mt-4 p-3 bg-green-50 rounded-lg">
              <h4 className="font-medium mb-1 text-green-800">Why This Works Better:</h4>
              <p className="text-sm text-green-700">
                By providing stronger penalties and terminating episodes at boundaries, the agent learns to avoid them much more effectively. The visual feedback helps understand the learning process.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
};

// Render the React component to the DOM
ReactDOM.render(<NeuralNetworkDemo />, document.getElementById('root'));
</script>
</body>
</html>
